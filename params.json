{
  "name": "Simple cloudtrail searcher",
  "tagline": "Simplest AWS CloudTrail logs parser using Elasticsearch",
  "body": "# CloudTrail-Searcher-Setup\r\nMany administrators are facing an auditing problem because they can't search events older than 1 week in AWS CloudTrail console. Good new is that all logs are available in S3 bucket. But problem is **How to search in it?** I have an simple solution to this problem, [Elasticsearch](https://www.elastic.co/products/elasticsearch) can be used here as search engine to the job.\r\n\r\n## Why Elasticsearch?\r\n- Elasticsearch consumes json document as an input and CloudTrail logs saved in S3 are in json format. So **no conversions**.\r\n- It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.\r\n\r\n## Setup Overview\r\n- We are going to use Docker containers to setup and running Elasticsearch because of  universality. \r\n- Then we are going to download Cloudtrail logs from S3 to local machine.\r\n- Then will unzip and feed it to Elasticsearch.\r\n\r\n## Installing prerequisite : Docker and AWS cli\r\n- Install Docker on your OS using [Docker official guide](https://docs.docker.com/engine/installation/).\r\n- Install AWS cli following [AWS official guide](http://docs.aws.amazon.com/cli/latest/userguide/installing.html).\r\n- And start both programs.\r\n\r\n## Making Elasticsearch container \r\nAs we want to use [head plugin](https://github.com/mobz/elasticsearch-head) (that will provides web gui), we have to include it in Elasticsearch docker image. Here we are making a [Dockerfile](https://docs.docker.com/engine/reference/builder/) to build a custom image [from official Elasticsearch docker image](https://hub.docker.com/_/elasticsearch/) and then installing head plugin in that image.\r\n```sh\r\n# mkdir ~/es-head\r\n# cd ~/es-head\r\n# cat > Dockerfile <<EOF\r\n>FROM elasticsearch:2.3\r\n>\r\n>RUN /usr/share/elasticsearch/bin/plugin install mobz/elasticsearch-head\r\n>EOF\r\n# \r\n```\r\nNow build a Docker image named “elasticsearch-head” from above Dockerfile(make sure that this directory should not contain anything other than Dockerfile).\r\n```sh\r\n# ls \r\nDockerfile\r\n# docker build -t elasticsearch-head:1 .\r\n```\r\nBelow command is use to create a [data only container](https://docs.docker.com/engine/tutorials/dockervolumes/#/creating-and-mounting-a-data-volume-container) from above custom image that will print \"DATA only ES\" and then exits. We will use this container indirectly to store data of our actual service to /es_data/DATA on host.\r\n```sh\r\n# mkdir -p /es_data/DATA\r\n# docker run --name es-cloudtrail_DATA -v \"/es_data/DATA\":/usr/share/elasticsearch/data elasticsearch:2.3 echo \"DATA only ES\"\r\nDATA only ES\r\n#\r\n```\r\nFrom [Docker run](https://docs.docker.com/v1.8/reference/commandline/run/), discription of above command.\r\n```\r\nUsage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...]\r\n\r\nRun a command in a new container\r\n--name=\"\"                     Assign a name to the container\r\n-v, --volume=[]               Bind mount a volume\r\n```\r\nFinally we are actually launching Elasticsearch container that will be used for searching CloudTrail logs.\r\n```sh \r\n# docker run -d --volumes-from es-cloudtrail_DATA -p 9200:9200 -p 9300:9300 --name es-cloudtrail  elasticsearch-head:1 -Des.node.name=\"es-cloudtrail\"\r\n```\r\n```\r\nUsage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...]\r\n\r\nRun a command in a new container\r\n-d, --detach=false            Run container in background and print container ID\r\n--name=\"\"                     Assign a name to the container\r\n--volumes-from=[]             Mount volumes from the specified container(s)\r\n-p, --publish=[]              Publish a container's port(s) to the host\r\n```\r\n## Downloading CloudTrail logs from S3\r\n- Create an IAM user with read only access to s3.\r\n- Configure AWS cli using [document](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html) for above created IAM user.\r\n```sh\r\n# aws configure --profile cloudtrail\r\nAWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE\r\nAWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\r\nDefault region name [None]: us-east-1\r\nDefault output format [None]: ENTER\r\n```\r\n- Now we are downloading all CloudTail logs from S3 using below command, make sure that enough space is available.\r\n```sh\r\n# mkdir /es_data/traillogs\r\n# aws s3 --profile cloudtrail cp s3://s3-traillogs-bucket-name/AWSLogs /es_data/traillogs/ --recursive\r\n```\r\n> Inplace of \"s3-traillogs-bucket-name\", use your actual s3 bucket where traillogs are saved.\r\n\r\n## Sending logs to Elasticsearch\r\n- Unzip all downloaded cloudtrail logs.\r\n```sh\r\n# gunzip -rv /es_data/traillogs\r\n```\r\nWe can use many things to send logs to Elasticsearch but simplest solution is to use [HTTP PUT/POST](https://www.elastic.co/guide/en/elasticsearch/guide/current/index-doc.html) request as we have documents already in json format.\r\n\r\nSo now we are going to make a simple script that will do this jobs.\r\n- First we will make a list of all traillogs file names with their full path. This is as simle is\r\n```sh\r\n# find /es_data/traillogs -name '*.json' -type f > /es_data/logs.list\r\n```\r\n- Now we will prefix a HTTP POST request for elasticsearch to  each line of above created logs.list file. This will make a POST request for each traillogs to elasticsearch.\r\n```sh\r\n# sed 's/^/curl -XPOST http://localhost:9200/AWS-account-name/old -d @/' /es_data/logs.list > /es_data/logs.sh\r\n```\r\n> here we are making a HTTP POST request to Elasticsearch on port 9200 with [Index and type](https://www.elastic.co/guide/en/elasticsearch/guide/current/index-doc.html#_autogenerating_ids) as \"AWS-account-name\"(your AWS account name to identify easily) and \"old\"(useful in future).\r\n\r\n- Now make it executable and run.\r\n```sh\r\n# chmod u+x /es_data/logs.sh\r\n# nohup bash /es_data/logs.sh &\r\n```\r\n- For space management, gzip again all downloaded traillogs.\r\n```sh\r\n# gzip -rv /es_data/traillogs\r\n```\r\n**Now setup is complete and we are ready to search.**\r\n## Searching\r\nNow elasticsearch can be accessed by [below URL](http://localhost:9200/_plugin/head).\r\n```\r\nhttp://localhost:9200/_plugin/head\r\n```\r\nFollow [Elasticsearch documentation](https://www.elastic.co/guide/en/elasticsearch/guide/current/search-in-depth.html) and [head plugin](https://mobz.github.io/elasticsearch-head/) for more on searching.\r\n## Future\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}